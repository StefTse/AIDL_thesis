{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07a0289",
   "metadata": {},
   "source": [
    "### BERT regressor (norm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979306f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id split     V     A     D text\n",
      "8281  easy_money_13624_13628   dev  2.78  2.89  2.78  NaN\n",
      "Data shapes: (8022,) (892,) (991,) (8022, 3) (892, 3) (991, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset, check and remove null values\n",
    "data_01 = pd.read_csv('Emo_Bank_VAD.csv')\n",
    "null_rows = data_01[data_01['text'].isnull()]\n",
    "print(null_rows)\n",
    "data_01 = data_01.dropna(subset=['text'])\n",
    "data_01.isnull().sum()\n",
    "\n",
    "# Normalize columns 'A', 'V', and 'D'\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "data_01[['A', 'V', 'D']] = scaler.fit_transform(data_01[['A', 'V', 'D']])\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(data_01[\"text\"], data_01[[\"V\", \"A\", \"D\"]], test_size=0.1, shuffle=True, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=1)\n",
    "print(\"Data shapes:\", x_train.shape, x_val.shape, x_test.shape, y_train.shape, y_val.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c50f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and pad\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 170\n",
    "x_train_pad = tokenizer(x_train.tolist(), padding='max_length', truncation=True, max_length=170, return_tensors=\"tf\")\n",
    "x_val_pad = tokenizer(x_val.tolist(), padding='max_length', truncation=True, max_length=170, return_tensors=\"tf\")\n",
    "x_test_pad = tokenizer(x_test.tolist(), padding='max_length', truncation=True, max_length=170, return_tensors=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2cebfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load BERT model \n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    # BERT input/output layers\n",
    "    input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
    "    pooled_output = bert_output.pooler_output  \n",
    "\n",
    "    # Dense layer for regression\n",
    "    dense = Dense(64, activation='relu')(pooled_output)  \n",
    "    output = Dense(3, activation='tanh')(dense)  # Output layer for 3 values (V, A, D), using 'tanh' to stay in range [-1, 1]\n",
    "\n",
    "    # Define the complete model\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instantiate the model\n",
    "model_BERT = create_model()\n",
    "\n",
    "\n",
    "# Compile the model \n",
    "model_BERT.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), \n",
    "              loss='mean_squared_error', \n",
    "              metrics=['mae']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fc4e207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 13:13:04.836388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502/502 [==============================] - ETA: 0s - loss: 0.0646 - mae: 0.1869"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 13:31:10.426137: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502/502 [==============================] - 1157s 2s/step - loss: 0.0646 - mae: 0.1869 - val_loss: 0.0293 - val_mae: 0.1321\n",
      "Epoch 2/6\n",
      "502/502 [==============================] - 1629s 3s/step - loss: 0.0327 - mae: 0.1363 - val_loss: 0.0580 - val_mae: 0.1914\n",
      "Epoch 3/6\n",
      "502/502 [==============================] - 1854s 4s/step - loss: 0.0284 - mae: 0.1279 - val_loss: 0.0293 - val_mae: 0.1324\n",
      "Epoch 4/6\n",
      "502/502 [==============================] - 2013s 4s/step - loss: 0.0351 - mae: 0.1412 - val_loss: 0.0311 - val_mae: 0.1286\n",
      "Epoch 5/6\n",
      "502/502 [==============================] - 2451s 5s/step - loss: 0.0262 - mae: 0.1224 - val_loss: 0.0245 - val_mae: 0.1170\n",
      "Epoch 6/6\n",
      "502/502 [==============================] - 2806s 6s/step - loss: 0.0278 - mae: 0.1259 - val_loss: 0.0288 - val_mae: 0.1284\n",
      "31/31 [==============================] - 136s 4s/step - loss: 0.0292 - mae: 0.1298\n",
      "Test MSE: 0.029166067019104958, Test MAE: 0.12983328104019165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 16:33:48.712909: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 144s 4s/step\n",
      "Mean Squared Error: 0.02916606507144547, Mean Absolute Error: 0.1298332695289107\n"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "history = model_BERT.fit(\n",
    "    x={'input_ids': x_train_pad['input_ids'], 'attention_mask': x_train_pad['attention_mask']},\n",
    "    y=y_train,\n",
    "    validation_data=({'input_ids': x_val_pad['input_ids'], 'attention_mask': x_val_pad['attention_mask']}, y_val),\n",
    "    epochs=6,  \n",
    "    batch_size=16  \n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_mae = model_BERT.evaluate({'input_ids': x_test_pad['input_ids'], 'attention_mask': x_test_pad['attention_mask']}, y_test)\n",
    "print(f\"Test MSE: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model_BERT.predict({'input_ids': x_test_pad['input_ids'], 'attention_mask': x_test_pad['attention_mask']})\n",
    "\n",
    "# Compute additional evaluation metrics like MSE and MAE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}, Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f88fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 79s 3s/step\n",
      "Test Dataset Evaluation:\n",
      "RMSE for V: 0.15\n",
      "RMSE for A: 0.18\n",
      "RMSE for D: 0.18\n",
      "\n",
      "MAE for V: 0.12\n",
      "MAE for A: 0.14\n",
      "MAE for D: 0.14\n",
      "\n",
      "MSE for V: 0.02\n",
      "MSE for A: 0.03\n",
      "MSE for D: 0.03\n",
      "\n",
      "Test Data Comparison:\n",
      "               V         A         D  V_pred  A_pred  D_pred\n",
      "0  5.882353e-02 -0.076923  0.008264    0.06   -0.24   -0.04\n",
      "1  2.220446e-16 -0.076923  0.256198   -0.13   -0.14   -0.13\n",
      "2 -2.705882e-01  0.092308  0.008264   -0.07    0.03   -0.11\n",
      "3  2.941176e-01 -0.076923  0.090909   -0.04   -0.02   -0.03\n",
      "4 -1.176471e-01  0.000000 -0.074380    0.19   -0.07    0.06\n"
     ]
    }
   ],
   "source": [
    "# Ensure that y_test is converted to a numpy array\n",
    "y_test_np = y_test.to_numpy()\n",
    "\n",
    "# Predict on the tokenized test set using the trained BERT model\n",
    "y_pred = model_BERT.predict({'input_ids': x_test_pad['input_ids'], 'attention_mask': x_test_pad['attention_mask']})\n",
    "\n",
    "# Convert predictions to numpy array\n",
    "y_pred_np = np.array(y_pred)\n",
    "\n",
    "# Define RMSE function\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Evaluate performance\n",
    "def evaluate_performance(y_true, y_pred):\n",
    "    metrics = {}\n",
    "    metrics['RMSE_V'] = rmse(y_true[:, 0], y_pred[:, 0])\n",
    "    metrics['RMSE_A'] = rmse(y_true[:, 1], y_pred[:, 1])\n",
    "    metrics['RMSE_D'] = rmse(y_true[:, 2], y_pred[:, 2])\n",
    "    \n",
    "    metrics['MAE_V'] = mean_absolute_error(y_true[:, 0], y_pred[:, 0])\n",
    "    metrics['MAE_A'] = mean_absolute_error(y_true[:, 1], y_pred[:, 1])\n",
    "    metrics['MAE_D'] = mean_absolute_error(y_true[:, 2], y_pred[:, 2])\n",
    "    \n",
    "    metrics['MSE_V'] = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n",
    "    metrics['MSE_A'] = mean_squared_error(y_true[:, 1], y_pred[:, 1])\n",
    "    metrics['MSE_D'] = mean_squared_error(y_true[:, 2], y_pred[:, 2])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate performance metrics for the test dataset\n",
    "test_metrics = evaluate_performance(y_test_np, y_pred_np)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Test Dataset Evaluation:\")\n",
    "print(f'RMSE for V: {round(test_metrics[\"RMSE_V\"], 2)}')\n",
    "print(f'RMSE for A: {round(test_metrics[\"RMSE_A\"], 2)}')\n",
    "print(f'RMSE for D: {round(test_metrics[\"RMSE_D\"], 2)}\\n')\n",
    "print(f'MAE for V: {round(test_metrics[\"MAE_V\"], 2)}')\n",
    "print(f'MAE for A: {round(test_metrics[\"MAE_A\"], 2)}')\n",
    "print(f'MAE for D: {round(test_metrics[\"MAE_D\"], 2)}\\n')\n",
    "print(f'MSE for V: {round(test_metrics[\"MSE_V\"], 2)}')\n",
    "print(f'MSE for A: {round(test_metrics[\"MSE_A\"], 2)}')\n",
    "print(f'MSE for D: {round(test_metrics[\"MSE_D\"], 2)}\\n')\n",
    "\n",
    "# Convert predictions to DataFrame and round to two decimal places\n",
    "y_pred_df = pd.DataFrame(y_pred_np, columns=[\"V_pred\", \"A_pred\", \"D_pred\"]).round(2)\n",
    "\n",
    "# Concatenate real and predicted values for comparison\n",
    "test_comparison = pd.concat([y_test.reset_index(drop=True), y_pred_df], axis=1)\n",
    "\n",
    "# Print comparison table\n",
    "print(\"Test Data Comparison:\\n\", test_comparison.head())\n",
    "\n",
    "# Save the comparison table to a CSV file\n",
    "test_comparison.to_csv(\"test_comparison_BERT_regression_03.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
