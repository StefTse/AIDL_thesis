{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 import pandas as pd\
import numpy as np\
import tensorflow as tf\
from sklearn.model_selection import train_test_split\
from tensorflow.keras.optimizers import Adam\
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\
from sklearn.metrics import classification_report\
\
# Load data\
Emotions_Data = pd.read_csv('Emotions_Data.csv')\
\
# Replace emotion labels with numerical values\
Emotions_Data_num = Emotions_Data.replace(\{"sadness":0, "anger":1, "love":2, "surprise":3, "fear":4, \
                                           "happiness":5, "neutral":6, "worry":7, "fun":8, "hate":9, \
                                           "relief":10, "gratitude":11, "disapproval":12, "amusement":13,\
                                           "disappointment":14, "admiration":15, "realization":16,\
                                           "annoyance":17, "confusion":18, "optimism":19, "curiosity":20, \
                                           "caring":21, "approval":22, "joy":23, "excitement":24\})\
\
# Split dataset into train, validation, and test sets\
x_train, x_test, y_train, y_test = train_test_split(Emotions_Data_num["Text"], Emotions_Data_num["Emotion"], \
                                                    test_size=0.1, shuffle=True, random_state=1)\
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, shuffle=True, random_state=1)\
\
# Tokenize and pad with explicit max_length\
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\
\
x_train_pad = tokenizer.batch_encode_plus(x_train.tolist(), return_tensors='np', padding='max_length', max_length=128)\
x_val_pad = tokenizer.batch_encode_plus(x_val.tolist(), return_tensors='np', padding='max_length', max_length=128)\
x_test_pad = tokenizer.batch_encode_plus(x_test.tolist(), return_tensors='np', padding='max_length', max_length=128)\
\
# Convert BatchEncoding to NumPy arrays\
x_train_array = \{key: x_train_pad[key] for key in x_train_pad\}\
x_val_array = \{key: x_val_pad[key] for key in x_val_pad\}\
\
# One-hot-encode classes\
n_classes = len(np.unique(y_train))\
y_train_np = y_train.values\
y_val_np = y_val.values\
y_test_np = y_test.values\
\
y_train_enc = tf.keras.utils.to_categorical(y_train_np, n_classes)\
y_val_enc = tf.keras.utils.to_categorical(y_val_np, n_classes)\
y_test_enc = tf.keras.utils.to_categorical(y_test_np, n_classes)\
\
# Create DistilBERT model\
model_distilBERT = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=25)\
\
# Freeze all layers except the classification layer\
for layer in model_distilBERT.layers[:-1]:\
    layer.trainable = False\
\
# Compile model\
optimizer = Adam(learning_rate=1e-5)\
loss = tf.keras.losses.CategoricalCrossentropy()\
metric = tf.keras.metrics.CategoricalAccuracy()\
model_distilBERT.compile(optimizer=optimizer, loss=loss, metrics=metric)\
\
# Train the model without callbacks initially\
history = model_distilBERT.fit(\
    x_train_array,\
    y_train_enc,\
    batch_size=16,\
    epochs=100,\
    validation_data=(x_val_array, y_val_enc)\
)\
}